{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff1710da-5965-4d7c-ad72-b5bb3f091f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import policy\n",
    "import gym\n",
    "from platformdirs import user_desktop_dir\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "\n",
    "import random, math\n",
    "import numpy as np\n",
    "from utils import pref_save, pref_load\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from reinforce_PPORLHF import reinforce_rwd2go_PPO_RLHF\n",
    "\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98738791-b9ce-42e7-80df-15918085a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"MountainCarContinuous-v0\"\n",
    "env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016f84e-a10b-46d5-8df9-7a71ee03d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy1 = PPO.load('./policies/ppo_mountain_ctn_final.zip')\n",
    "policy2 = PPO.load('./policies/ppo_mountain_ctn_10000_steps.zip')\n",
    "pref_data = pref_load('./pref_data/pref_data_100_MountainCarContinuous-v0.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fb815a6e-4ff6-4e64-9b1c-d9ff8ec6ae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8 — avg loss: 461.0858\n",
      "Epoch 2/8 — avg loss: 384.6009\n",
      "Epoch 3/8 — avg loss: 308.2630\n",
      "Epoch 4/8 — avg loss: 235.1981\n",
      "Epoch 5/8 — avg loss: 168.4429\n",
      "Epoch 6/8 — avg loss: 110.3193\n",
      "Epoch 7/8 — avg loss: 63.1294\n",
      "Epoch 8/8 — avg loss: 27.4724\n"
     ]
    }
   ],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, state_size=2, action_size=1, hidden_size=32):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)\n",
    "\n",
    "    def predict_reward(self, state, action):\n",
    "        state = state\n",
    "        action = torch.tensor(action).reshape(1,1)  \n",
    "        \n",
    "        state_action = torch.cat((state, action), dim=1)\n",
    "        reward = self.forward(state_action).cpu()\n",
    "        return reward\n",
    "    \n",
    "\n",
    "lr        = 3e-2\n",
    "epochs    = 8\n",
    "\n",
    "reward_model = RewardModel(state_size=2, action_size=1)\n",
    "\n",
    "optimizer = torch.optim.Adam(reward_model.parameters(), lr=lr)\n",
    "\n",
    "def trajectory_reward(reward_model, states, actions):\n",
    "    total_reward = torch.tensor(0., device=device)\n",
    "    for s, a in zip(states, actions):\n",
    "        s_t = torch.tensor(s, dtype=torch.float32, device=device)\n",
    "        reward = reward_model.predict_reward(s_t.unsqueeze(0), a).squeeze(0)\n",
    "        total_reward += reward.squeeze(0)\n",
    "    return total_reward\n",
    "\n",
    "losses_reward_model = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for s0, tau_plus, tau_minus in pref_data:\n",
    "        reward_plus = trajectory_reward(reward_model, tau_plus[\"states\"], tau_plus[\"actions\"])\n",
    "        reward_minus = trajectory_reward(reward_model, tau_minus[\"states\"], tau_minus[\"actions\"])\n",
    "        stacked = torch.stack([reward_plus, reward_minus])\n",
    "        log_Z   = torch.logsumexp(stacked, dim=0)\n",
    "        total_loss += - (reward_plus - log_Z)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(pref_data)\n",
    "    losses_reward_model.append(avg_loss.detach().numpy().item())\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{epochs} — avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8dc8411e-1eb9-4709-a294-159aacbc6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sb3Wrapper(nn.Module):\n",
    "    def __init__(self, model, std_=3.0):\n",
    "        super(sb3Wrapper,self).__init__()\n",
    "        self.extractor = model.policy.mlp_extractor\n",
    "        self.policy_net = model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = model.policy.action_net\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(next(self.parameters()).device)\n",
    "        mean_act = self.forward(state_tensor)\n",
    "\n",
    "        std = torch.ones_like(mean_act) * 3\n",
    "        dist = Normal(mean_act, std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        \n",
    "        action_np = action.detach().squeeze(0).cpu().numpy()\n",
    "        return action_np, log_prob.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "917715dc-dc49-443f-b813-8abd96a55f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "policyCopied = sb3Wrapper(policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "91b6f764-3175-4003-b6c8-6d5fa6712ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#proba2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "71bffb0c-6bd6-41ca-95e8-6e4f2ff9862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sb3Wrapper(nn.Module):\n",
    "    def __init__(self, model, std_=10.0):\n",
    "        super(sb3Wrapper,self).__init__()\n",
    "        self.extractor = model.policy.mlp_extractor\n",
    "        self.policy_net = model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = model.policy.action_net\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(next(self.parameters()).device)\n",
    "        mean_act = self.forward(state_tensor)\n",
    "\n",
    "        std = torch.ones_like(mean_act) * 10\n",
    "        dist = Normal(mean_act, std)\n",
    "        action = dist.sample()\n",
    "\n",
    "        delta = 0.2\n",
    "        lower = action - delta\n",
    "        upper = action + delta\n",
    "        \n",
    "        cdf_upper = dist.cdf(upper)\n",
    "        cdf_lower = dist.cdf(lower)\n",
    "        \n",
    "        prob_interval = cdf_upper - cdf_lower\n",
    "        \n",
    "        log_prob = torch.log(prob_interval + 1e-10)\n",
    "            \n",
    "        action_np = action.detach().squeeze(0).cpu().numpy()\n",
    "        return action_np, log_prob.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "19016787-7e5f-4b2a-8f89-81d9d88328bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 100\tavg100: -6339.68\n",
      "Ep 200\tavg100: -7265.26\n",
      "Ep 300\tavg100: -7666.10\n",
      "Ep 400\tavg100: -8036.48\n",
      "Ep 500\tavg100: -8233.76\n",
      "Ep 600\tavg100: -9230.97\n",
      "Ep 700\tavg100: -9428.03\n",
      "Ep 800\tavg100: -9378.62\n",
      "Ep 900\tavg100: -9626.19\n"
     ]
    }
   ],
   "source": [
    "policyCopied = sb3Wrapper(policy2)\n",
    "opt1 = optim.Adam(policyCopied.parameters(), lr=1e-3)\n",
    "reward_model.eval()\n",
    "\n",
    "reward_evaluation_every=10\n",
    "losses, mean_returns, std_returns = reinforce_rwd2go_PPO_RLHF(env, policyCopied, opt1, reward_model, n_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4b599a8f-c44f-4a16-ba5e-e14a25966a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5393.29970061,  -5712.46297244,  -6886.96661506,  -3522.33361626,\n",
       "        -6628.95613077,  -6902.51355744,  -5863.53092378,  -5887.57982098,\n",
       "        -4635.32796411,  -7088.54656869,  -4611.7968164 ,  -5280.48155177,\n",
       "        -6015.49543261,  -6582.74921262,  -7612.94159208,  -9822.3370435 ,\n",
       "        -4489.8062188 ,  -9600.85280448,  -8039.94871635,  -6125.21295883,\n",
       "        -5989.72666766,  -8767.01499473,  -9003.08321822,  -5083.42619445,\n",
       "        -8281.76813405,  -3763.96004788,  -9751.30844791,  -6869.54999997,\n",
       "        -6257.14062601,  -9335.45634646,  -8162.45716704,  -9034.08646427,\n",
       "        -7226.03268554,  -9110.7390822 ,  -5814.09392934,  -4749.76913099,\n",
       "        -8485.97417231,  -8986.54212531, -10526.95213295,  -8469.21583391,\n",
       "       -10348.45247739,  -7935.31496262,  -6324.15043948,  -9564.5327546 ,\n",
       "        -8797.73413904,  -7338.57789092,  -9768.67332368,  -6726.93354003,\n",
       "        -7683.51502292,  -5811.75279539, -10038.41662882,  -5800.86336981,\n",
       "        -8829.02740212, -10265.12286306,  -7670.01270016, -10056.43631548,\n",
       "       -10003.62648593,  -6647.48623181,  -9064.71230553,  -7268.66929414,\n",
       "        -8308.56299532,  -6749.44005119,  -9119.65545539,  -8387.79133418,\n",
       "        -9645.33643196,  -9206.75285157, -10039.72714637,  -9394.85399601,\n",
       "        -9539.34671193,  -9531.51760109,  -8536.18218443,  -6026.96105346,\n",
       "       -10290.79230986, -10240.74802964,  -7600.75524985,  -8803.20529758,\n",
       "        -9923.9403284 ,  -6988.62885952,  -9736.82326006, -10184.71147061,\n",
       "        -9540.90435785, -10268.00436851,  -8073.06314609,  -9204.13562659,\n",
       "        -9790.97704849, -10021.84230773, -10108.86544361, -10312.28147774,\n",
       "        -9592.34936516,  -9759.76734869, -10251.85059099,  -9952.69876872,\n",
       "       -10936.16819822, -10191.39403172, -10290.5968478 , -11280.95985379,\n",
       "       -11196.19872605, -12921.49610749, -12501.56256753])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db807b-c4c3-4301-b8a4-5f9ab56f76d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
